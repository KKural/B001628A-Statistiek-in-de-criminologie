---
title: "Exercise II - Correlation Only (25 Questions)"
always_allow_html: true
output: 
  word_document:
    toc: true
    fig_caption: true
    reference_docx: "default"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.show = 'asis')
library(knitr)
library(kableExtra)
library(ggplot2)
library(dplyr)
library(flextable)
options(knitr.kable.NA = '')
```

# Basisbegrippen in de Correlation

## üìã OVERZICHTSTABEL - VRAGEN VOLGENS BLOOM'S TAXONOMIE (25 VRAGEN)

### REMEMBER LEVEL (5 vragen - 20%)
| Vraagnummer | Vraag Titel | Leerintentie |
|-------------|-------------|--------------|
| Q1 | What is correlation? | Definitie van correlatie herinneren |
| Q2 | What is a z-score? | Z-score definitie herinneren |
| Q3 | Key measures in correlation | Belangrijke correlatiematen herinneren |
| Q4 | Center in z-scores | Centrum van gestandaardiseerde verdeling herinneren |
| Q5 | Units don't change r | Invariantie van correlatie voor eenheden herinneren |

### UNDERSTAND LEVEL (8 vragen - 32%)
| Vraagnummer | Vraag Titel | Leerintentie |
|-------------|-------------|--------------|
| Q6 | Variable types for correlation | Variabeletypes voor correlatie begrijpen |
| Q7 | Correlation vs causation basics | Verschil correlatie-causatie begrijpen |
| Q8 | Interpreting correlation values | Correlatie-interpretatie begrijpen |
| Q9 | Direction of relationships | Richting van verbanden begrijpen |
| Q10 | What does correlation tell us? | Wat correlatie betekent begrijpen |
| Q11 | Anscombe's Quartet | Belang van visualisatie begrijpen |
| Q12 | Covariance vs correlation | Verschil covariantie-correlatie begrijpen |
| Q13 | Describe the pattern | Patroonbeschrijving begrijpen |

### APPLY LEVEL (3 vragen - 12%)
| Vraagnummer | Vraag Titel | Leerintentie |
|-------------|-------------|--------------|
| Q14 | Monotone vs. linear | Monotoon versus lineair toepassen |
| Q15 | Direction and strength | Richting en sterkte bepalen |
| Q16 | Weak positive | Zwak positieve correlatie herkennen |

### ANALYZE LEVEL (4 vragen - 16%)
| Vraagnummer | Vraag Titel | Leerintentie |
|-------------|-------------|--------------|
| Q17 | Correlation ‚â† Causation | Correlatie-causatie mythe analyseren |
| Q18 | Subgroups vs. overall r | Subgroep versus totaal analyseren |
| Q19 | Outlier impact | Impact van uitbijters analyseren |
| Q20 | Aggregation pitfall | Aggregatievalkuil analyseren |

### EVALUATE LEVEL (3 vragen - 12%)
| Vraagnummer | Vraag Titel | Leerintentie |
|-------------|-------------|--------------|
| Q21 | Evaluate conflicting study results | Tegenstellende onderzoeksresultaten evalueren |
| Q22 | Assess statistical claims | Statistische claims beoordelen |
| Q23 | Evaluate model assumptions | Modelveronderstellingen evalueren |

### CREATE LEVEL (2 vragen - 8%)
| Vraagnummer | Vraag Titel | Leerintentie |
|-------------|-------------|--------------|
| Q24 | Design a correlation study | Correlatiestudie ontwerpen |
| Q25 | Create research hypothesis | Onderzoekshypothese formuleren |

# Correlation ‚Äî Enhanced with Visual Learning

This assessment covers correlation concepts essential for criminology statistics. The file includes carefully structured questions across different cognitive levels, with visual examples, practical applications, and detailed feedback for optimal learning.

## Visual Learning Guide

### Understanding Correlation Patterns Through Scatterplots

```{r correlation-patterns, echo=FALSE, fig.width=12, fig.height=8}
# Generate example data for different correlation patterns
set.seed(123)
n <- 50

# Strong positive correlation (r ‚âà 0.85)
x1 <- rnorm(n, 50, 10)
y1 <- 0.85 * scale(x1)[,1] * 8 + rnorm(n, 60, 3)

# Strong negative correlation (r ‚âà -0.75)
x2 <- rnorm(n, 50, 10)
y2 <- -0.75 * scale(x2)[,1] * 8 + rnorm(n, 60, 4)

# Weak correlation (r ‚âà 0.15)
x3 <- rnorm(n, 50, 10)
y3 <- 0.15 * scale(x3)[,1] * 8 + rnorm(n, 60, 8)

# No correlation (r ‚âà 0)
x4 <- rnorm(n, 50, 10)
y4 <- rnorm(n, 60, 8)

# Create combined dataset
data_patterns <- data.frame(
  x = c(x1, x2, x3, x4),
  y = c(y1, y2, y3, y4),
  pattern = rep(c("Strong Positive (r ‚âà +0.85)", "Strong Negative (r ‚âà -0.75)", 
                  "Weak Positive (r ‚âà +0.15)", "No Correlation (r ‚âà 0)"), each = n)
)

# Create the visualization
ggplot(data_patterns, aes(x = x, y = y)) +
  geom_point(alpha = 0.7, size = 2, color = "steelblue") +
  geom_smooth(method = "lm", se = TRUE, color = "red", linewidth = 1) +
  facet_wrap(~pattern, ncol = 2, scales = "free") +
  labs(
    title = "Understanding Correlation Patterns in Criminology Data",
    subtitle = "How data points cluster around the trend line indicates correlation strength",
    x = "Predictor Variable (e.g., socioeconomic status)",
    y = "Outcome Variable (e.g., crime rate)",
    caption = "Note: Shaded area shows confidence interval around regression line"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 11),
    strip.text = element_text(size = 10, face = "bold"),
    axis.title = element_text(size = 10),
    plot.caption = element_text(size = 9, face = "italic")
  )
```

**Key Interpretation Points:**
- **Strong Positive**: Points cluster tightly around upward-sloping line
- **Strong Negative**: Points cluster tightly around downward-sloping line  
- **Weak Correlation**: Points scattered widely around trend line
- **No Correlation**: No discernible linear pattern, flat or chaotic arrangement

## Interpretation Guide for Criminology Research

### Effect Size Guidelines (Cohen's Conventions)
- **Small correlation**: r = .10 (1% of variance explained)
- **Medium correlation**: r = .30 (9% of variance explained)  
- **Large correlation**: r = .50 (25% of variance explained)

### Practical Significance vs Statistical Significance
- **Statistical significance**: p < .05 (relationship unlikely due to chance)
- **Practical significance**: Effect size large enough to matter in real-world applications
- **Example**: r = .15, p < .001 in large sample may be statistically significant but practically small

### Common Interpretation Pitfalls
1. **Correlation ‚â† Causation**: High correlation doesn't prove one variable causes another
2. **Third variables**: Alternative explanations may account for observed relationships
3. **Restriction of range**: Limited variability can artificially reduce correlations
4. **Outliers**: Extreme scores can inflate or deflate correlation coefficients
5. **Non-linear relationships**: Correlation assumes linear associations
---

# Correlation Questions (25 Total)

## REMEMBER LEVEL (5 vragen)

### Vraag Q1 (Remember)
**What is correlation?**

> **Hint:** Think about how two criminological variables move together ‚Äî for example, unemployment and crime rate ‚Äî not about cause and effect.

1) A measure of how much one variable causes another
"1" = "‚ùå Incorrect. This is a common misconception in criminological research. Correlation shows that two variables change together, but it does not prove that one causes the other. For instance, neighborhoods with high unemployment often have higher crime rates, but this may result from a third factor such as poverty, social disorganization, or weak informal control."

2) A statistical measure of the strength and direction of the relationship between two variables
"2" = "‚úÖ Correct! Correlation quantifies how strongly and in what direction two quantitative variables are related. A positive correlation (e.g., r = +0.68 between unemployment and property crime) means higher unemployment is associated with higher crime, while a negative correlation (e.g., r = ‚Äì0.55 between police visibility and disorder) means that as one increases, the other decreases."

3) The difference between two variables
"3" = "‚ùå Incorrect. Taking a difference (Y ‚Äì X) only measures a gap, not whether the variables vary together. For example, the difference between theft and assault rates says nothing about whether these crimes rise or fall together. Correlation examines co-variation ‚Äî how much both variables deviate from their means in the same or opposite direction."

4) A method to predict future values
"4" = "‚ùå Incorrect. That's regression analysis, not correlation. Regression allows criminologists to predict, for example, the expected crime rate for a given unemployment level. Correlation only summarizes how two variables are related at one point in time ‚Äî it cannot make predictions or prove causation."

---

### Vraag Q2 (Remember)
**What is a z-score?**

> **Hint:** Think about comparing crime statistics across different jurisdictions or time periods ‚Äî how do we know if a crime rate is unusually high or low relative to the typical pattern?

1) The raw score minus the mean  
"1" = "‚ùå Incorrect. This gives you a deviation score, which tells you how far above or below the mean a value is, but doesn't account for the variability in your data. For example, if the mean crime rate is 50 per 1,000 and a city has 60 per 1,000, the deviation is +10. But we can't judge if this is 'a lot' without knowing whether crime rates typically vary by ¬±2 or ¬±20 around the mean. The z-score standardizes this by dividing by the standard deviation."

2) A score that shows how many standard deviations a value is from the mean  
"2" = "‚úÖ Correct! Z-scores standardize values using the formula z = (X - Œº)/œÉ, allowing meaningful comparisons across different variables or time periods. For example, if a city's violent crime rate has z = +2.1, this means it's 2.1 standard deviations above the average for all cities ‚Äî an unusually high rate. Z-scores let us compare 'how unusual' different crime types are: a burglary rate with z = +1.5 and an assault rate with z = +0.8 both indicate above-average crime, but burglary is more extremely elevated."

3) The percentage above the mean  
"3" = "‚ùå Incorrect. Z-scores are expressed in standard deviation units, not percentages. However, z-scores can be converted to percentiles using the normal distribution. For instance, z = +1.0 corresponds to about the 84th percentile (meaning 84% of values fall below this point), but the z-score itself isn't a percentage. In criminology, this distinction matters when interpreting risk assessments or comparing crime statistics across jurisdictions."

4) The square of the original score  
"4" = "‚ùå Incorrect. Squaring a score would dramatically change its scale and lose information about whether the original value was above or below the mean. Z-scores preserve the relative position while standardizing the scale. If we squared crime rates, a city with 100 crimes per 1,000 would become 10,000 ‚Äî completely changing the meaning and making comparisons impossible."

---

### Vraag Q3 (Remember)
**What are the key measures in correlation analysis?**

> **Hint:** When studying the relationship between two crime-related variables, what statistics do you need to fully understand and report the association?

```{r correlation-key-measures, echo=FALSE, fig.width=8, fig.height=5}
# Create example correlation data for a criminology study
set.seed(123)
n <- 40
crime_data <- data.frame(
  Police_Presence = round(rnorm(n, mean=5, sd=1.2), 1),
  Drug_Offenses = round(rnorm(n, mean=25, sd=7) - 3*rnorm(n, mean=5, sd=1.2), 1),
  Violent_Crime = round(rnorm(n, mean=15, sd=5) - 2*rnorm(n, mean=5, sd=1.2), 1),
  Property_Crime = round(rnorm(n, mean=40, sd=10) - 2.5*rnorm(n, mean=5, sd=1.2), 1)
)

# Calculate correlation matrix
cor_matrix <- round(cor(crime_data), 2)

# Create a nice looking table for the correlation matrix
library(knitr)
library(kableExtra)
kable(cor_matrix, caption = "Correlation Matrix: Key Crime Variables", booktabs = TRUE)
```

**Example of Complete Correlation Reporting:**
- Police presence is negatively associated with drug offenses (r = -0.48, R¬≤ = 0.23, n = 40), indicating that 23% of the variance in drug offense rates is explained by police presence.
- Sample size (n = 40) provides adequate statistical power to detect moderate correlations.

1) Only the correlation coefficient (r)  
"1" = "‚ùå Incorrect. While r is the primary measure, reporting only r provides incomplete information for criminological research. You also need to know how much variance is explained (R¬≤) and whether your sample size (n) is large enough to trust the result. For example, r = 0.40 between police presence and crime reduction might seem meaningful, but if n = 8 cities, the result is unreliable. Professional criminological research requires reporting all three statistics."

2) Correlation coefficient (r), coefficient of determination (R¬≤), and sample size (n)  
"2" = "‚úÖ Correct! These three provide complete information about the relationship. The correlation coefficient (r) tells you the strength and direction (-1 to +1), R¬≤ tells you the percentage of variance explained (r¬≤ √ó 100%), and sample size (n) indicates reliability. For example: 'Police visibility and disorder showed a moderate negative correlation (r = -0.45, R¬≤ = 0.20, n = 150 neighborhoods), indicating that 20% of the variation in disorder can be explained by police presence levels.' This gives readers all the information needed to evaluate the finding."

3) Only the mean and standard deviation  
"3" = "‚ùå Incorrect. Means and standard deviations describe individual variables but don't capture the relationship between two variables. You could have identical means and standard deviations for unemployment and crime rates across two different studies, but completely different correlations (r = +0.80 vs. r = -0.20). Correlation analysis specifically examines how variables co-vary, which requires different statistics than basic descriptive measures."
4) The slope and intercept  

"4" = "‚ùå Incorrect. Slope and intercept are regression parameters, not correlation measures. While regression and correlation are related (slope = r √ó sy/sx), they serve different purposes. Correlation summarizes the strength of association; regression provides prediction equations. In criminology, you might report correlation when studying whether two crime types tend to occur together, but regression when predicting future crime rates from demographic factors."

---

### Vraag Q4 (Remember)
**Where is the 'center of gravity' in a standardized (z‚Äëscore) scatterplot?**

> **Hint:** When you convert crime variables to z-scores (e.g., standardized burglary rates and standardized unemployment rates), where do the means intersect?

1) (0, 0) ‚Äî intersection of mean lines  
"1" = "‚úÖ Correct! When variables are standardized to z-scores, both means become 0, so the bivariate center is (0,0). This is extremely useful in criminological research for interpreting correlations visually. In a z-score scatterplot of unemployment vs. crime rates, the point (0,0) represents average unemployment and average crime. Points in the upper-right quadrant (both positive) represent above-average unemployment AND above-average crime, while points in the lower-left (both negative) represent below-average unemployment AND below-average crime. The clustering of points around this (0,0) center helps visualize the correlation strength."

2) (1, 1) ‚Äî one standard deviation point  
"2" = "‚ùå Incorrect. The point (1,1) represents a location where both variables are one standard deviation above their means ‚Äî this is not the center of the distribution. In a criminology context, if you're looking at standardized police presence and crime rates, (1,1) would represent a community with both high police presence (z = +1) and high crime (z = +1) ‚Äî potentially indicating that police are deployed reactively to high-crime areas. This might be an interesting data point, but it's not the center of gravity."
3) The densest part of the cloud  

"3" = "‚ùå Incorrect. The 'densest part' refers to the mode (most frequent area), which isn't necessarily the mean-center. In criminological data, the mode might be skewed by clustering effects ‚Äî for example, if many rural counties have similar low crime rates, creating a dense cluster away from the mean. The center of gravity is determined by means, not by where most data points cluster. This distinction matters when interpreting outliers like major cities with extreme crime rates."

4) The most frequent observed point  
"4" = "‚ùå Incorrect. This describes the mode, not the mean. In criminological research, data rarely clusters at exactly one point due to measurement precision and natural variation. For example, even if many neighborhoods have 'about 5 crimes per 1,000,' the exact values might be 4.8, 5.1, 4.9, etc. The mean-center (0,0) in standardized space represents the balance point of all data, not the most common value."

---

### Vraag Q5 (Remember)
**You switch X from meters to centimeters. What happens to r(X,Y)?**

> **Hint:** If you measure police patrol distance in meters vs. centimeters, or crime rates per 1,000 vs. per 100,000 residents, how does this affect the correlation with other variables?

1) r becomes 100√ó larger  
"1" = "‚ùå Incorrect. Units don't affect correlation magnitude. This is a crucial property for criminological research where variables often use different scales. For example, the correlation between 'years of education' and 'arrests per 1,000 residents' would be the same as the correlation between 'years of education' and 'arrests per 100,000 residents.' The scale change doesn't alter the underlying relationship strength. If changing units affected correlation, comparative criminological research across countries with different measurement systems would be impossible."

2) r becomes 100√ó smaller  
"2" = "‚ùå Incorrect. Again, correlation is scale-invariant. Whether you measure patrol car response time in seconds, minutes, or hours, the correlation with crime clearance rates remains identical. This property makes correlation particularly valuable for international criminological comparisons ‚Äî researchers can use data measured in different units without mathematical adjustment. The relationship strength between two variables is independent of how those variables are scaled."

3) r stays exactly the same  
"3" = "‚úÖ Correct! Correlation is unit-free and invariant to linear rescaling. This is because correlation standardizes both variables during calculation ‚Äî it measures how variables move together relative to their own variability, regardless of units. In criminology, this means you can compare correlations across studies that use different measurement scales. For example, studies measuring crime rates per capita vs. per square kilometer can still have comparable correlation coefficients with demographic factors. This invariance property makes correlation a universal measure of association."
4) r becomes negative  

"4" = "‚ùå Incorrect. Unit changes cannot flip the direction of a relationship. If increased police presence is associated with decreased crime (negative correlation), this relationship direction remains the same whether you measure presence in officers per block, per square kilometer, or per 1,000 residents. Only the scale changes, not the fundamental pattern. A sign flip would indicate a completely different substantive relationship, which unit conversion cannot cause."

---

## UNDERSTAND LEVEL (8 vragen)

### Vraag Q6 (Understand)
**For which types of variables can you calculate Pearson correlation?**

> **Hint:** Consider whether you can meaningfully calculate correlation between different types of criminological variables ‚Äî can you correlate gender with crime rate? Prison sentence length with recidivism likelihood?

1) Only categorical variables  
"1" = "‚ùå Incorrect. Pearson correlation requires numeric variables where mathematical operations (addition, subtraction, averaging) are meaningful. Categorical variables like 'type of crime' (violent, property, drug) or 'court disposition' (guilty, not guilty, plea bargain) don't have inherent numerical values. You can't meaningfully calculate an average of 'violent' and 'property' crimes. For categorical variables, you would use different measures like Cram√©r's V or Chi-square tests to examine associations."
2) Both variables must be continuous (interval or ratio level)  

"2" = "‚úÖ Correct! Pearson correlation requires both variables to be measured at interval or ratio levels ‚Äî numeric variables where the distances between values are meaningful and consistent. In criminology, examples include: crime rates (ratio), sentence length in months (ratio), age at first arrest (ratio), number of prior convictions (ratio), and scales measuring attitudes toward police (interval). These variables allow meaningful mathematical operations necessary for calculating means, standard deviations, and covariances that form the basis of Pearson correlation."

3) One categorical and one continuous variable  
"3" = "‚ùå Incorrect. When you have one categorical and one continuous variable, you use point-biserial correlation (for binary categories) or eta correlation (for multiple categories). For example, if studying the relationship between gender (categorical: male/female) and number of arrests (continuous), you would use point-biserial correlation. If examining the relationship between type of neighborhood (categorical: urban/suburban/rural) and crime rate (continuous), you might use ANOVA or eta correlation rather than Pearson correlation."

4) Any type of variables  
"4" = "‚ùå Incorrect. Different correlation measures exist for different variable combinations. Pearson correlation is specifically for continuous variables. Other measures include: Spearman's rho for ranked data, phi coefficient for two binary variables, Cram√©r's V for two categorical variables, and point-biserial for one binary and one continuous variable. Using the wrong correlation type can lead to meaningless results or serious misinterpretation of relationships in criminological research."

---

### Vraag Q7 (Understand)
**Why doesn't correlation prove causation?**

> **Hint:** Consider why a strong correlation between ice cream sales and drowning deaths, or between number of police officers and crime rates, doesn't necessarily mean one causes the other.

1) Correlation is always too weak to prove anything  
"1" = "‚ùå Incorrect. Even very strong correlations (r = 0.90 or higher) don't establish causation. The issue isn't the strength of the correlation but the logic of causal inference. In criminology, you might find a very strong correlation (r = 0.85) between the number of security cameras in a neighborhood and crime rates, but this doesn't prove cameras cause crime. The high correlation could exist because cameras are installed in response to high crime, not because cameras cause crime. Correlation strength and causal evidence are completely separate issues."

2) Third variables, reverse causation, or coincidence might explain the relationship  
"2" = "‚úÖ Correct! Correlation cannot establish causal direction or rule out alternative explanations. In criminology: (1) Third variables ‚Äî poverty might cause both low education and high crime, creating a spurious correlation between education and crime; (2) Reverse causation ‚Äî do more police reduce crime, or does more crime lead to hiring more police? (3) Coincidence ‚Äî two variables might correlate by chance, especially with small samples. Establishing causation requires controlled experiments, longitudinal designs, natural experiments, or sophisticated statistical techniques that can isolate causal effects from these alternative explanations."

3) Only experiments can show any relationships  
"3" = "‚ùå Incorrect. Correlation clearly shows that relationships exist ‚Äî it just can't determine whether they're causal. Many important criminological relationships can't be studied experimentally for ethical reasons (you can't randomly assign people to criminal careers). Correlational studies provide valuable evidence about associations, risk factors, and patterns. While experiments provide the strongest causal evidence, observational studies using techniques like instrumental variables, regression discontinuity, or natural experiments can also provide compelling causal evidence when experiments aren't feasible."
4) Correlation is the same as causation  

"4" = "‚ùå Incorrect. This represents exactly the misconception that criminologists must avoid. Media reports often claim 'studies show X causes Y' when the research only demonstrated correlation. For example, finding that neighborhoods with more streetlights have less crime doesn't prove streetlights prevent crime ‚Äî safer neighborhoods might simply be more likely to install streetlights. Understanding this distinction is crucial for evidence-based criminal justice policy, as implementing interventions based on correlational evidence alone can waste resources or even backfire."

---

### Vraag Q8 (Understand)
**How do you interpret a correlation of r = 0.75?**

```{r positive-correlation, echo=FALSE, fig.width=10, fig.height=5}
# Generate example data for positive correlation of 0.75
set.seed(123)
n <- 80
x <- rnorm(n, 50, 10)
y <- 0.75 * scale(x)[,1] * 10 + rnorm(n, 60, 6)

# Create data frame
pos_cor_data <- data.frame(
  Variable_X = x,
  Variable_Y = y
)

# Calculate correlation and R-squared
r_value <- round(cor(x, y), 2)
r_squared <- round(r_value^2 * 100, 1)

# Create visualization
ggplot(pos_cor_data, aes(x = Variable_X, y = Variable_Y)) +
  geom_point(color = "steelblue", size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", formula = y ~ x, se = TRUE, color = "darkred") +
  labs(
    title = "Visualizing a Correlation of r = 0.75",
    subtitle = paste0("Strong Positive Correlation (R¬≤ = ", r_squared, "%)"),
    x = "Variable X (e.g., Parental Monitoring)",
    y = "Variable Y (e.g., Academic Performance)",
    caption = "As Variable X increases, Variable Y tends to increase (positive relationship)"
  ) +
  annotate("text", x = min(x) + 5, y = max(y) - 5,
           label = paste0("Key Interpretation:\n",
                         "‚Ä¢ Direction: Positive\n",
                         "‚Ä¢ Strength: Strong relationship\n",
                         "‚Ä¢ Variance Explained: ", r_squared, "%\n",
                         "‚Ä¢ Effect Size: Large (Cohen's convention)"),
           hjust = 0, size = 4, fontface = "bold") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12)
  )
```

> **Hint:** Think about what this correlation strength means for the relationship between two crime-related variables ‚Äî is this a weak, moderate, or strong association?

1) Weak positive relationship  
"1" = "‚ùå Incorrect. An r = 0.75 represents a strong relationship, not weak. In criminological research, correlations around 0.75 are considered quite substantial. For comparison, the correlation between height and weight in adults is typically around 0.70-0.80. If you found r = 0.75 between neighborhood poverty and crime rates, this would indicate a strong positive association ‚Äî as poverty increases, crime rates tend to increase substantially. This level of correlation suggests the variables share about 56% of their variance (0.75¬≤ = 0.56), which is considerable in social science research."

2) Strong positive relationship
"2" = "‚úÖ Correct! Values around 0.7-0.8 indicate strong positive associations in social science research. An r = 0.75 means that as one variable increases, the other tends to increase substantially in a predictable pattern. In criminology, this might represent the relationship between unemployment rates and property crime, or between drug arrests and drug overdose deaths in a community. The relationship is strong enough to be practically meaningful for policy and prediction, while still acknowledging that 44% of the variance remains unexplained by this single correlation."

3) Perfect negative relationship
"3" = "‚ùå Incorrect. This interpretation has two major errors: (1) r = 0.75 is positive, not negative ‚Äî the value is above zero, indicating variables move in the same direction; (2) 0.75 is strong but not perfect ‚Äî a perfect correlation would be r = ¬±1.00. A perfect negative correlation (r = -1.00) means that as one variable increases, the other decreases in exact proportion, with no exceptions. In criminology, perfect correlations are virtually impossible due to the complexity of human behavior and measurement error."

4) No relationship
"4" = "‚ùå Incorrect. An r = 0.75 indicates a strong relationship, not the absence of one. 'No relationship' would be indicated by r ‚âà 0.00. To put r = 0.75 in perspective, many important relationships in criminology are weaker than this ‚Äî for example, the correlation between individual criminal attitudes and actual criminal behavior might be r = 0.30-0.50. Finding r = 0.75 between two crime-related variables would be considered a substantial and practically important association worthy of further investigation and potential policy consideration."

---

### Vraag Q9 (Understand)
**What does the direction of a correlation tell you?**

> **Hint:** Think about whether variables move together in the same direction (both increase together) or in opposite directions (one increases while the other decreases) in criminological research.

1) How strong the relationship is  
"1" = "‚ùå Incorrect. Direction (positive vs. negative) and strength (how close to ¬±1.00) are separate aspects of correlation. The direction is indicated by the sign (+ or -), while strength is indicated by the absolute value. For example, r = +0.80 and r = -0.80 have equal strength but opposite directions. In criminology, both the direction and strength matter: r = -0.70 between police patrol frequency and burglary rates (strong negative ‚Äî more patrols, fewer burglaries) tells a different story than r = +0.70 between unemployment and burglary rates (strong positive ‚Äî more unemployment, more burglaries)."

2) Whether variables increase together (positive) or one increases as the other decreases (negative)
"2" = "‚úÖ Correct! Direction indicates the pattern of co-variation between variables. Positive correlations (+) mean variables move in the same direction: as neighborhood income increases, property values increase; as drug availability increases, drug arrests increase. Negative correlations (-) mean variables move in opposite directions: as police visibility increases, street crime decreases; as education levels increase, recidivism rates decrease. Understanding direction is crucial for criminological theory and policy ‚Äî it tells you whether factors work as risk factors (positive correlation with crime) or protective factors (negative correlation with crime)."

3) Whether the relationship is causal  
"3" = "‚ùå Incorrect. Direction cannot determine causality ‚Äî that requires additional evidence beyond correlation. A negative correlation between police presence and crime rates could mean: (1) police reduce crime (causal), (2) high crime areas get more police (reverse causation), or (3) both are influenced by a third factor like neighborhood wealth (spurious correlation). Similarly, a positive correlation between broken windows and crime doesn't prove broken windows cause crime ‚Äî both might result from neighborhood disinvestment. Causal direction requires experimental evidence, temporal sequencing, or sophisticated statistical controls."

4) How many observations are in the dataset  
"4" = "‚ùå Incorrect. The correlation direction is independent of sample size (n). You could have r = +0.60 with n = 30 cities or n = 3,000 cities ‚Äî the positive direction remains the same. Sample size affects the reliability and statistical significance of the correlation, but not its direction. However, larger samples do provide more stable estimates of the correlation's direction and magnitude, which is why criminological studies often try to include many jurisdictions or time periods when possible."

---

### Vraag Q10 (Understand)
**What does a correlation tell us?**

> **Hint:** Focus on what correlation actually measures ‚Äî how two criminological variables tend to vary together, not what causes what.

1) How much one variable causes the other  
"1" = "‚ùå Incorrect. Correlation measures association, not causation. This is a critical distinction in criminological research. For example, a correlation between single-parent households and juvenile delinquency doesn't tell us that single-parent families cause delinquency ‚Äî both might be influenced by poverty, neighborhood factors, or other variables. Correlation tells us these variables tend to occur together, but determining causal relationships requires additional evidence like experimental studies, natural experiments, or longitudinal research with proper controls. Confusing correlation with causation leads to misguided policies and interventions."

2) The direction and strength of a linear relationship between two variables  
"2" = "‚úÖ Correct! Correlation quantifies how consistently and strongly two variables move together in a straight-line pattern. The correlation coefficient (r) captures both direction (positive/negative sign) and strength (absolute value from 0 to 1). In criminology, this might tell us that unemployment rates and property crime rates tend to increase together (positive direction) in a moderately strong pattern (e.g., r = +0.45). This information is valuable for understanding risk factors, developing prevention strategies, and identifying communities that might benefit from interventions, even though it doesn't prove causation."

3) The difference between two group means
"3" = "‚ùå Incorrect. Comparing group means uses t-tests or ANOVA, not correlation. For example, comparing average recidivism rates between offenders who received job training vs. those who didn't would use a t-test to examine the difference in means. Correlation examines how two continuous variables co-vary (e.g., how individual recidivism risk scores relate to actual reoffending frequency). The statistical approaches and interpretations are completely different ‚Äî group comparisons focus on differences between categories, while correlation focuses on linear relationships between continuous measures."
4) The total variance in one variable  

"4" = "‚ùå Incorrect. Total variance describes the spread of a single variable around its mean, calculated as the standard deviation squared. Correlation examines shared variance between two variables ‚Äî how much they vary together. The coefficient of determination (R¬≤ = r¬≤) tells us the proportion of variance in one variable that's predictable from the other variable. For example, if r = 0.60 between neighborhood poverty and crime rates, then R¬≤ = 0.36, meaning 36% of the variance in crime rates is associated with poverty levels, while 64% remains unexplained."

---

### Vraag Q11 (Understand)
**What is the main lesson of Anscombe's Quartet?**

```{r anscombes-quartet, echo=FALSE, fig.width=10, fig.height=8, warning=FALSE, message=FALSE}
library(tidyr)
library(dplyr)

# Recreate Anscombe's Quartet
anscombe_x1 <- c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5)
anscombe_y1 <- c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68)
anscombe_x2 <- c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5)
anscombe_y2 <- c(9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74)
anscombe_x3 <- c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5)
anscombe_y3 <- c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73)
anscombe_x4 <- c(8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8)
anscombe_y4 <- c(6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89)

# Calculate statistics (they're the same for all datasets)
corr <- round(cor(anscombe_x1, anscombe_y1), 2)
mean_x <- round(mean(anscombe_x1), 2)
mean_y <- round(mean(anscombe_y1), 2)
sd_x <- round(sd(anscombe_x1), 2)
sd_y <- round(sd(anscombe_y1), 2)

# Combine the data
anscombe_df <- data.frame(
  x1 = anscombe_x1, y1 = anscombe_y1,
  x2 = anscombe_x2, y2 = anscombe_y2,
  x3 = anscombe_x3, y3 = anscombe_y3,
  x4 = anscombe_x4, y4 = anscombe_y4
)

# Transform to long format for easier plotting
anscombe_long <- anscombe_df %>%
  mutate(id = row_number()) %>%
  gather(key, value, -id) %>%
  separate(key, c("axis", "dataset"), sep = 1) %>%
  spread(axis, value) %>%
  select(dataset, id, x, y)

# Create descriptive names for the datasets
dataset_names <- c(
  "1" = "Linear Pattern",
  "2" = "Curved Pattern",
  "3" = "Outlier Effect",
  "4" = "Extreme Leverage Point"
)

# Plot the data
ggplot(anscombe_long, aes(x = x, y = y)) +
  geom_point(size = 3, color = "steelblue") +
  geom_smooth(method = "lm", se = FALSE, color = "red", formula = y ~ x) +
  facet_wrap(~ dataset, ncol = 2, labeller = labeller(dataset = dataset_names)) +
  labs(
    title = "Anscombe's Quartet: Same Statistics, Different Patterns",
    subtitle = paste0("All datasets have: r = ", corr, ", Mean x = ", mean_x, 
                      ", Mean y = ", mean_y, ", SD x = ", sd_x, ", SD y = ", sd_y),
    caption = "Correlation and regression can be misleading without visual inspection",
    x = "Variable X",
    y = "Variable Y"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 10),
    strip.text = element_text(size = 11, face = "bold")
  )
```

**Key Lesson:** Always visualize your data. These four datasets have identical summary statistics (mean, standard deviation, correlation coefficient, regression line), but they tell completely different stories that are only revealed through visualization.

> **Hint:** Four datasets can share identical summaries yet have very different shapes.

1) Correlation tells the whole story
"1" = "‚ùå **Incorrect.** Anscombe's Quartet powerfully demonstrates that correlation coefficients alone can be deeply misleading. All four datasets share the identical correlation (r = 0.82), yet they represent fundamentally different relationships:

- One shows a proper linear relationship
- Another reveals a curved (nonlinear) pattern
- The third contains an influential outlier
- The fourth shows a leverage point creating an artificial association

This illustrates why relying solely on correlation values without examining the actual data structure can lead to serious misinterpretations in research. Summary statistics compress complex patterns into single numbers, potentially masking critical insights that could change your entire interpretation of the relationship between variables."

2) Always plot your data first
"2" = "‚úÖ **Correct!** Anscombe's Quartet is one of the most powerful demonstrations in statistics of why visualization should be a first step in analysis, not an afterthought. 

The brilliance of this example is that all four datasets have:
- Identical means for X and Y
- Identical standard deviations
- Identical correlation coefficients (r = 0.82)
- Identical regression lines (same slope and intercept)

Yet when plotted, they reveal completely different patterns that dramatically change their interpretation. Only through visualization can you detect nonlinear relationships, identify influential outliers, or discover distinct patterns that numbers alone hide. This principle applies across all research fields - always visualize before calculating."

3) Graphs are less reliable than statistics
"3" = "‚ùå **Incorrect.** Anscombe's Quartet demonstrates precisely the opposite point: visualizations reveal critical insights that summary statistics alone cannot capture. The four datasets share identical statistical properties (same means, standard deviations, correlation, and regression equation), yet they tell completely different stories when graphed.

Far from being less reliable, graphs often reveal the true nature of relationships that numbers might obscure. Modern statistical practice emphasizes the importance of visualization throughout the analysis process for good reason - it helps researchers:
- Identify patterns that equations miss
- Detect outliers and influential points
- Recognize when relationships are nonlinear
- Avoid misinterpreting statistical summaries"

4) Linear relationships are the most common
"4" = "‚ùå **Incorrect.** Anscombe's Quartet makes no claim about the prevalence of linear versus nonlinear relationships in real-world data. Rather, it warns against assuming linearity without visual verification.

The quartet demonstrates that identical correlation coefficients can arise from both linear and nonlinear patterns, showing why assumptions about relationship shapes must be verified through visualization. In many research fields, nonlinear relationships are quite common and theoretically important:
- Dose-response curves in pharmacology
- Diminishing returns in economics
- Threshold effects in ecology
- J-curved relationships in criminology

The key lesson concerns visualization's importance in revealing actual patterns, not about which relationship types predominate in practice."

---

### Vraag Q12 (Understand)
**Which statement about covariance and correlation is true?**

```{r covariance-correlation, echo=FALSE, fig.width=10, fig.height=6}
# Generate two datasets with same correlation but different scales
set.seed(456)
n <- 50

# Dataset 1: Small scale variables
x1 <- rnorm(n, 5, 1)
y1 <- 0.7 * scale(x1)[,1] * 1 + rnorm(n, 5, 0.6)
cov1 <- round(cov(x1, y1), 2)
cor1 <- round(cor(x1, y1), 2)

# Dataset 2: Large scale variables
x2 <- x1 * 100  # Same pattern but 100x scale
y2 <- y1 * 100  # Same pattern but 100x scale
cov2 <- round(cov(x2, y2), 0)
cor2 <- round(cor(x2, y2), 2)

# Create combined data frame
small_scale <- data.frame(x = x1, y = y1, dataset = "Small Scale Variables")
large_scale <- data.frame(x = x2, y = y2, dataset = "Large Scale Variables (x100)")
all_data <- rbind(small_scale, large_scale)

# Create comparison table
comparison <- data.frame(
  Measure = c("Covariance", "Correlation"),
  `Small Scale` = c(paste0(cov1), paste0(cor1)),
  `Large Scale` = c(paste0(cov2), paste0(cor2)),
  Unit_Dependence = c("Depends on units (changes with scale)", "Unit-free (stays constant)")
)

# Create visualization with two plots
p1 <- ggplot(small_scale, aes(x = x, y = y)) +
  geom_point(color = "steelblue", size = 2.5) +
  geom_smooth(method = "lm", se = FALSE, color = "darkred", formula = y ~ x) +
  labs(
    title = paste0("Small Scale: Covariance = ", cov1, ", Correlation = ", cor1),
    x = "Variable X (small units)",
    y = "Variable Y (small units)"
  ) +
  theme_minimal()

p2 <- ggplot(large_scale, aes(x = x, y = y)) +
  geom_point(color = "steelblue", size = 2.5) +
  geom_smooth(method = "lm", se = FALSE, color = "darkred", formula = y ~ x) +
  labs(
    title = paste0("Large Scale: Covariance = ", cov2, ", Correlation = ", cor2),
    x = "Variable X (large units)",
    y = "Variable Y (large units)"
  ) +
  theme_minimal()

# Combine plots using gridExtra
library(gridExtra)
grid.arrange(p1, p2, nrow = 2,
             top = grid::textGrob("Covariance vs. Correlation: Effect of Scale", 
                                  gp = grid::gpar(fontsize = 14, fontface = "bold")))
```

**Key Comparison:**
- **Covariance**: Changes with scale/units (0.70 vs 7,000), making comparisons difficult
- **Correlation**: Standardized (-1 to +1) and unit-free (0.70 in both cases)
- **Interpretation**: Correlation allows meaningful comparison across different variable scales

> **Hint:** Watch for units and ranges.

1) Covariance is unit‚Äëfree and bounded ‚àí1 to +1  

"1" = "‚ùå Incorrect. You've confused covariance with correlation. Covariance is NOT unit-free and is NOT bounded between -1 and +1. Covariance values depend on the measurement units (e.g., dollars, kilograms, years) and can range from negative infinity to positive infinity. For example, the covariance between income (measured in thousands of dollars) and education (measured in years) might be 4.5, but if income were measured in dollars instead, the covariance would be 4,500 - a very different number representing the same relationship strength."

2) Correlation is unit‚Äëfree and always between ‚àí1 and +1  

"2" = "‚úÖ Correct! Correlation (r) standardizes the relationship by dividing covariance by the product of standard deviations, creating a unit-free measure that always falls between -1 and +1. This standardization is why we can meaningfully compare correlation values across different variable pairs regardless of their original measurement scales. For example, r = 0.7 represents the same strength of relationship whether we're correlating height with weight or test scores with study time. This bounded, standardized nature makes correlation the preferred measure for comparing relationship strength across different variable pairs."

3) Both are unbounded  

"3" = "‚ùå Incorrect. While covariance is indeed unbounded (ranging from negative infinity to positive infinity), correlation is specifically designed to be bounded between -1 and +1. This is a fundamental difference between these two measures. Correlation's bounded nature comes from its normalization process, dividing the covariance by the product of standard deviations. This mathematical transformation ensures that regardless of how large the covariance might be, the correlation coefficient will always fall within the standardized -1 to +1 range, making it easier to interpret and compare across different analyses."

4) Both depend on measurement scale  

"4" = "‚ùå Incorrect. A key distinction between these measures is that covariance depends on measurement scale while correlation does not. If you change units (e.g., from meters to centimeters), the covariance will change proportionally, but correlation remains unchanged. This is why correlation is preferred for comparative analysis - it provides a standardized measure that isn't affected by arbitrary choices in measurement units. For instance, measuring temperature in Celsius versus Fahrenheit would yield different covariance values but identical correlation coefficients when examining relationships with other variables."

---

### Vraag Q13 (Understand)
**"As X increases, Y tends to ‚Ä¶" ‚Äî choose the best completion for a slightly increasing pattern.**

> **Hint:** Use the standard sentence for a weak upward trend.

1) ‚Ä¶ decrease, strongly  

"1" = "‚ùå **Incorrect.** This answer mischaracterizes both the direction and magnitude of the relationship:

- **Direction error**: A 'slightly increasing pattern' means Y increases as X increases, not decreases
- **Magnitude error**: 'Strongly' contradicts the described 'slight' increase

A strong negative correlation (r ‚âà -0.8) would appear as a steep downward slope with points tightly clustered around the line‚Äîthe complete opposite of what's described. In criminology, this would be like claiming that as community policing increases, crime rates drop dramatically, when the data actually shows a weak tendency for crime rates to rise slightly with more policing (perhaps due to better reporting)."

2) ‚Ä¶ increase, weakly  

"2" = "‚úÖ **Correct!** This precisely captures both the direction and magnitude of a slightly increasing pattern:

- **Direction**: 'Increase' correctly identifies the positive relationship (as X rises, Y tends to rise)
- **Magnitude**: 'Weakly' appropriately characterizes the modest strength (typically r ‚âà 0.10 to 0.30)

On a scatterplot, this would appear as an upward-sloping trend line with considerable scatter of points around it. For example, in research on social factors and crime, you might find that as neighborhood median income increases, reporting of certain offenses weakly increases‚Äîa real pattern, but with substantial variation and many other influencing factors. Using precise language like this helps readers form an accurate mental picture of the relationship strength."

3) ‚Ä¶ increase, very strongly  

"3" = "‚ùå **Incorrect.** While this answer correctly identifies the direction (positive relationship), it severely misrepresents the strength. A 'slightly increasing pattern' indicates a weak relationship, not a very strong one.

In correlation terminology:
- 'Very strongly' suggests r > 0.9 with points tightly clustered along a steep line
- 'Slightly' suggests r ‚âà 0.1-0.3 with considerable scatter around a shallow trend line

This distinction matters greatly in research interpretation. For example, claiming that education level very strongly increases crime reporting when the actual relationship is slight would lead to overemphasis on education in policy decisions when other factors might be more influential. Precision in describing both direction and magnitude is essential in statistical communication."

4) ‚Ä¶ stay the same  

"4" = "‚ùå **Incorrect.** This description contradicts the premise of a 'slightly increasing pattern.' 

Saying Y 'stays the same' as X increases indicates:
- No relationship between variables (r ‚âà 0)
- A horizontal line on a scatterplot
- No predictive value of X for Y

The question specifically describes a slightly increasing pattern, meaning Y does tend to increase (even if modestly) as X increases. In criminological terms, this would be like claiming there's no relationship between age and certain types of offending behavior when the data actually shows a slight positive trend. Such mischaracterization could lead to incorrect theoretical conclusions and misguided policy recommendations."

---

# APPLY LEVEL (3 vragen)

### Vraag Q14 (Apply)
**The relationship is curved but monotonically increasing. Which correlation should you choose?**

> **Hint:** Distinguish "linear" from "monotone".

1) Pearson correlation  
"1" = "‚ùå **Incorrect.** Pearson measures linear association, so it underestimates curved but monotone relationships. For example, crime rates might rise quickly at first with unemployment but level off later ‚Äî a curve Pearson can‚Äôt capture well."
2) Spearman correlation  

"2" = "‚úÖ **Correct!** Spearman uses ranks and captures monotone relationships even when the pattern bends. If social disorder steadily increases as income inequality rises, Spearman œÅ reflects that consistent direction despite curvature."

3) Both are equally appropriate  
"3" = "‚ùå **Incorrect.** Only Spearman handles nonlinearity properly. Pearson assumes a straight line, so their results can differ substantially when the pattern is curved."

4) Neither  
"4" = "‚ùå **Incorrect.** Spearman is the best option here since it recognizes ordered but nonlinear changes, such as gradual saturation effects between stress and aggression."

---

### Vraag Q15 (Apply)
**A straight trend line slopes upward; points cluster tightly. Which description fits best?**

> **Hint:** Combine direction (sign) with strength (spread).

```{r correlation-strength, echo=FALSE, fig.width=10, fig.height=7}
# Create four datasets with different correlation patterns
set.seed(123)
n <- 60

# Create data with different correlation strengths
create_data <- function(strength, direction="positive") {
  x <- rnorm(n, 50, 10)
  
  if(direction == "positive") {
    factor <- strength
  } else {
    factor <- -strength
  }
  
  noise <- (1-abs(strength)) * 10
  y <- factor * scale(x)[,1] * 10 + rnorm(n, 50, noise)
  
  if(strength == 0) {
    y <- rnorm(n, 50, 10)  # No relationship
  }
  
  return(data.frame(x=x, y=y))
}

# Generate four datasets
strong_pos <- create_data(0.85, "positive")
moderate_pos <- create_data(0.45, "positive")
weak_pos <- create_data(0.15, "positive")
strong_neg <- create_data(0.85, "negative")

# Calculate actual correlations
cor_strong_pos <- round(cor(strong_pos$x, strong_pos$y), 2)
cor_moderate_pos <- round(cor(moderate_pos$x, moderate_pos$y), 2)
cor_weak_pos <- round(cor(weak_pos$x, weak_pos$y), 2)
cor_strong_neg <- round(cor(strong_neg$x, strong_neg$y), 2)

# Combine datasets with labels
strong_pos$strength <- paste0("Strong Positive (r = ", cor_strong_pos, ")")
moderate_pos$strength <- paste0("Moderate Positive (r = ", cor_moderate_pos, ")")
weak_pos$strength <- paste0("Weak Positive (r = ", cor_weak_pos, ")")
strong_neg$strength <- paste0("Strong Negative (r = ", cor_strong_neg, ")")

# Combine all datasets
all_data <- rbind(
  transform(strong_pos, strength_order = 1),
  transform(moderate_pos, strength_order = 2),
  transform(weak_pos, strength_order = 3),
  transform(strong_neg, strength_order = 4)
)

# Convert to factor with specific order
all_data$strength <- factor(all_data$strength, 
                           levels = c(paste0("Strong Positive (r = ", cor_strong_pos, ")"),
                                      paste0("Moderate Positive (r = ", cor_moderate_pos, ")"),
                                      paste0("Weak Positive (r = ", cor_weak_pos, ")"),
                                      paste0("Strong Negative (r = ", cor_strong_neg, ")")))

# Create visualization
ggplot(all_data, aes(x = x, y = y)) +
  geom_point(color = "steelblue", size = 2, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "darkred", formula = y ~ x) +
  facet_wrap(~ strength, ncol = 2) +
  labs(
    title = "Visual Patterns in Correlations of Different Strengths",
    subtitle = "Notice how tightly points cluster around the trend line",
    x = "Variable X",
    y = "Variable Y"
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 14, face = "bold")
  )
```

**Key Visual Characteristics:**
- **Strong positive correlation**: Upward sloping line with points clustered tightly around it
- **Moderate positive correlation**: Upward sloping line with moderate scatter around it
- **Weak positive correlation**: Upward sloping line with wide scatter around it
- **Strong negative correlation**: Downward sloping line with points clustered tightly

1) Strong positive  

"1" = "‚úÖ **Correct!** A tight cloud of points clustered closely around an upward-sloping line is the visual signature of a strong positive correlation (typically r > 0.7). This pattern indicates both the direction of the relationship (positive - as X increases, Y reliably increases) and its strength (strong - points adhere closely to the linear pattern with minimal scatter). The visualization shows minimal deviation from the trend line, suggesting a highly predictable relationship where knowing the X value allows accurate prediction of the Y value. For example, in criminological research, you might see such a pattern between neighborhood social disorganization scores and crime rates - as disorganization increases, crime rates increase in a highly predictable manner. The tight clustering of points indicates minimal influence from other factors, suggesting a powerful and consistent relationship."

2) Moderate positive  
"2" = "‚ùå **Incorrect.** The pattern is tighter than ‚Äòmoderate.‚Äô Moderate relationships show more scatter around the line."

3) Weak negative  
"3" = "‚ùå **Incorrect.** The direction is clearly positive, not negative; higher X corresponds to higher Y."

4) No linear pattern  
"4" = "‚ùå **Incorrect.** The points align well with a line, indicating a clear linear pattern rather than randomness."

---

### Vraag Q16 (Apply)
**Which r value best matches 'weak positive'?**

> **Hint:** Use common interpretation guidelines.

1) r = 0.12  

"1" = "‚úÖ **Correct!** This represents a weak positive association (typically r values between 0.1-0.3 are considered weak). In research, this magnitude indicates a real but modest relationship where the variables share only a small amount of variance (about 1.4% since R¬≤ = 0.12¬≤ = 0.014). For example, the relationship between education level and fear of crime might show such a correlation - slightly higher education tends to correspond with slightly higher concern about specific crimes, but the relationship is subtle enough that many other factors clearly play more important roles. When interpreting weak correlations, it's essential to consider both statistical significance and practical significance, as even small effects can be meaningful in large samples or certain contexts."

2) r = 0.56  
"2" = "‚ùå **Incorrect.** This would be considered a moderate relationship, stronger than ‚Äòweak.‚Äô"

3) r = ‚àí0.72  
"3" = "‚ùå **Incorrect.** That‚Äôs a strong negative relationship, not weak positive, suggesting that as one variable rises, the other falls sharply."

4) r = 0.93  
"4" = "‚ùå **Incorrect.** This is an extremely strong positive relationship, nearly perfect ‚Äî far stronger than ‚Äòweak.‚Äô"

---

## ANALYZE LEVEL (4 vragen)

### Vraag Q17 (Analyze)
**Headline: "Ice cream sales and drownings are strongly correlated." What's a plausible explanation?**

```{r correlation-causation, echo=FALSE, fig.width=10, fig.height=6}
# Create simulated data with common cause (temperature)
set.seed(123)
n <- 100

# Generate temperature data (common cause)
temperature <- runif(n, 10, 35)  # Temperature in Celsius

# Generate ice cream sales and drownings based on temperature
ice_cream_sales <- 1000 + 80 * temperature + rnorm(n, 0, 300)  # Units sold
drownings <- 1 + 0.3 * temperature + rnorm(n, 0, 2)  # Number of incidents

# Calculate correlation
correlation <- round(cor(ice_cream_sales, drownings), 2)

# Combine into data frame
data <- data.frame(
  Temperature = temperature,
  Ice_Cream_Sales = ice_cream_sales,
  Drownings = drownings,
  Month = factor(sample(month.abb, n, replace=TRUE), levels=month.abb)
)

# Create visualization
p1 <- ggplot(data, aes(x=Ice_Cream_Sales, y=Drownings)) +
  geom_point(aes(color=Temperature), size=3, alpha=0.8) +
  scale_color_gradient(low="blue", high="red") +
  geom_smooth(method="lm", se=TRUE, color="darkblue") +
  labs(
    title=paste0("Correlation between Ice Cream Sales and Drownings (r = ", correlation, ")"),
    subtitle="Colored by temperature (¬∞C)",
    x="Ice Cream Sales (units sold)",
    y="Drowning Incidents"
  ) +
  theme_minimal()

# Create small multiples showing the real relationship
p2 <- ggplot(data, aes(x=Temperature)) +
  geom_point(aes(y=Ice_Cream_Sales/max(Ice_Cream_Sales)*10), color="darkgreen") +
  geom_smooth(aes(y=Ice_Cream_Sales/max(Ice_Cream_Sales)*10), 
              method="lm", se=FALSE, color="darkgreen") +
  geom_point(aes(y=Drownings), color="darkred") +
  geom_smooth(aes(y=Drownings), method="lm", se=FALSE, color="darkred") +
  labs(
    title="Common Cause: Temperature Affects Both Variables",
    x="Temperature (¬∞C)",
    y="Scale"
  ) +
  scale_y_continuous(
    name="Drownings",
    sec.axis=sec_axis(~.*max(data$Ice_Cream_Sales)/10, name="Ice Cream Sales")
  ) +
  theme_minimal() +
  theme(
    axis.title.y.left=element_text(color="darkred"),
    axis.title.y.right=element_text(color="darkgreen")
  )

# Display plots
grid.arrange(p1, p2, nrow=2)
```

**Key Concept**: This is a classic example of a confounding variable (temperature/season) causing both outcomes, creating a spurious correlation. As temperature increases, both ice cream sales and swimming activity increase, leading to more drownings. The correlation between ice cream sales and drownings isn't causal - you wouldn't reduce drownings by restricting ice cream sales.

> **Hint:** Think of a third variable (confounder) driving both.

1) Ice cream causes drownings  

"1" = "‚ùå Incorrect. This response represents the fundamental error of assuming causation from correlation. While ice cream sales and drownings are indeed correlated (as shown in the top graph), this does not mean one causes the other. This is a classic example used in statistics courses to illustrate spurious correlations. The visualization clearly shows that both variables increase with temperature (bottom graph), suggesting that temperature is the common cause. If ice cream caused drownings, then reducing ice cream consumption would reduce drownings - an absurd conclusion that fails to recognize the actual mechanism at work. In any field of research, mistaking correlation for causation can lead to severely flawed conclusions and ineffective interventions."

2) Drownings cause higher ice cream sales  

"2" = "‚ùå Incorrect. This answer incorrectly assumes not just causation from correlation, but also proposes an illogical direction of causality. The temporal sequence alone makes this implausible - ice cream sales typically occur before drowning incidents, not as a result of them. Additionally, there is no plausible mechanism by which drownings would drive increased ice cream consumption. The visualization clearly shows that both variables are independently affected by temperature. This kind of reverse causality error is particularly problematic in research, as it can lead to completely misguided interventions (imagine trying to reduce ice cream sales by preventing drownings). Always consider the logical sequence of events and plausible mechanisms when examining correlational relationships."

3) Temperature/season increases both  

"3" = "‚úÖ Correct! This answer correctly identifies temperature as a confounding variable (or common cause) that independently affects both ice cream sales and drowning incidents. As shown in the bottom visualization, higher temperatures lead to both increased ice cream consumption and more swimming activity, which in turn increases drowning risk. This creates a spurious correlation between ice cream and drownings - they appear related but have no causal connection to each other. This example perfectly illustrates why researchers must always consider potential third variables that might explain observed correlations. Understanding such confounding relationships is crucial across all research fields, from public health to economics to social sciences, as it prevents misattribution of causality and leads to more effective interventions."

4) There is no relationship at all  

"4" = "‚ùå Incorrect. While there is no causal relationship between ice cream sales and drownings, there is indeed a statistical association (correlation) between them, as clearly shown in the top graph. The visualization shows a strong positive correlation (r = 0.66), which is a real statistical relationship even though it doesn't indicate causation. Dismissing this correlation entirely would be just as erroneous as assuming it indicates causation. The correct approach is to acknowledge the correlation while investigating its true explanation (in this case, the common cause of temperature). In scientific research, understanding the nature of relationships - whether causal or non-causal - is essential for building accurate models and drawing valid conclusions."

---

### Vraag Q18 (Analyze)
**A scatterplot colors points by 'neighborhood type'. How is the overall correlation r typically computed?**

> **Hint:** Compare subgroup calculations versus full sample.

```{r subgroup-correlation, echo=FALSE, fig.width=10, fig.height=5}
# Create simulated data for two neighborhood types
set.seed(123)

# Urban neighborhoods
n_urban <- 30
income_urban <- runif(n_urban, 20000, 60000)  # Lower income range
health_urban <- 40 + 0.0005 * income_urban + rnorm(n_urban, 0, 5)  # Health score

# Suburban neighborhoods
n_suburban <- 30
income_suburban <- runif(n_suburban, 50000, 100000)  # Higher income range
health_suburban <- 60 + 0.0002 * income_suburban + rnorm(n_suburban, 0, 5)  # Higher baseline health

# Combine data
income_all <- c(income_urban, income_suburban)
health_all <- c(health_urban, health_suburban)
neighborhood_type <- c(rep("Urban", n_urban), rep("Suburban", n_suburban))

# Calculate correlations
cor_urban <- round(cor(income_urban, health_urban), 2)
cor_suburban <- round(cor(income_suburban, health_suburban), 2)
cor_all <- round(cor(income_all, health_all), 2)

# Create data frame for plotting
plot_data <- data.frame(
  Income = income_all,
  Health = health_all,
  Neighborhood = factor(neighborhood_type)
)

# Create the plot
ggplot(plot_data, aes(x = Income, y = Health, color = Neighborhood)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
  geom_smooth(aes(group = 1), method = "lm", se = FALSE, 
              color = "black", linetype = "dashed", formula = y ~ x) +
  scale_color_manual(values = c("darkblue", "darkgreen")) +
  labs(
    title = "Income vs Health Score by Neighborhood Type",
    subtitle = paste0("Urban: r = ", cor_urban, ", Suburban: r = ", cor_suburban, 
                     ", Overall: r = ", cor_all, " (ignoring colors)"),
    x = "Income ($)",
    y = "Health Score",
    color = "Neighborhood Type"
  ) +
  annotate("text", x = 25000, y = 80, 
           label = "The default correlation (r) ignores\ncolors/groups and treats all points as one dataset",
           hjust = 0, size = 4, fontface = "bold") +
  theme_minimal()
```

1) Using only the largest subgroup  

"1" = "‚ùå Incorrect. Standard correlation calculations do not selectively use only the largest subgroup and ignore other data points. This approach would discard valuable information and create a biased picture of the relationship. The visualization shows both urban and suburban neighborhoods; choosing only the largest group would ignore roughly half the data points. In statistical analysis across all disciplines, discarding data without a methodological justification (such as removing outliers or invalid responses) is generally considered inappropriate. Pearson's correlation coefficient is designed to use all available valid data points, and artificially restricting the calculation to a subset would produce misleading results that don't represent the complete dataset."

2) As the average of subgroup correlations  

"2" = "‚ùå Incorrect. The overall correlation coefficient is not calculated by simply averaging the correlation coefficients of subgroups. As shown in the visualization, the urban group has r = 0.46, the suburban group has r = 0.33, but the overall correlation is r = 0.78, which is clearly not the average of 0.46 and 0.33. This is because the overall correlation captures not just the within-group patterns but also the between-group differences. The formula for Pearson's correlation uses all data points treated as one unified dataset, calculating covariance and standard deviations across the entire sample. This fundamental statistical principle applies across all research domains and is a key reason why understanding subgroup structures is important in correlation analysis."

3) Using all points together, ignoring colors  

"3" = "‚úÖ Correct! By default, Pearson's correlation coefficient (r) is calculated using all data points as a single dataset, completely ignoring any subgroup distinctions such as the colors in a scatterplot. As demonstrated in the visualization, the calculation treats both urban and suburban neighborhoods as one combined dataset, ignoring their categorical distinction. The formula for r involves the covariance of X and Y divided by the product of their standard deviations, and these calculations use all data points. This is precisely why correlation can sometimes be misleading when significant subgroup differences exist‚Äîa phenomenon related to Simpson's paradox. In any statistical software package, the default correlation function applies this approach unless specifically instructed to calculate separate correlations by group."

4) As a weighted average of subgroup r's  

"4" = "‚ùå Incorrect. The standard Pearson correlation is not calculated as a weighted average of subgroup correlations. While there are methods in meta-analysis that use weighted averaging of correlation coefficients from different studies, this is not how a single correlation coefficient is calculated for a dataset with subgroups. As the visualization shows, the overall r = 0.78 is substantially different from what any reasonable weighting of the subgroup correlations (r = 0.46 and r = 0.33) would produce. The overall correlation is higher because it captures the between-group pattern (suburban neighborhoods having both higher incomes and higher health scores compared to urban neighborhoods). This statistical principle holds true regardless of the specific research field or subject matter being studied."

---

### Vraag Q19 (Analyze)
**In a positive trend, which outlier position most reduces r?**

```{r outlier-impact, echo=FALSE, fig.width=10, fig.height=7, warning=FALSE}
# Create base data with positive correlation
set.seed(123)
n <- 30
x_base <- rnorm(n, 50, 10)
y_base <- 0.7 * scale(x_base)[,1] * 8 + rnorm(n, 60, 5)

# Create datasets with different outliers
x_outlier1 <- c(x_base, 100)  # high-high (upper right)
y_outlier1 <- c(y_base, 100)

x_outlier2 <- c(x_base, 0)    # low-low (lower left)
y_outlier2 <- c(y_base, 0)

x_outlier3 <- c(x_base, 100)  # high-low (lower right)
y_outlier3 <- c(y_base, 0)

x_outlier4 <- c(x_base, 0)    # low-high (upper left)
y_outlier4 <- c(y_base, 100)

# Calculate correlations
cor1 <- round(cor(x_outlier1, y_outlier1), 2)
cor2 <- round(cor(x_outlier2, y_outlier2), 2)
cor3 <- round(cor(x_outlier3, y_outlier3), 2)
cor4 <- round(cor(x_outlier4, y_outlier4), 2)
cor_base <- round(cor(x_base, y_base), 2)

# Combine into one dataframe for plotting
outlier_data <- data.frame(
  dataset = rep(c("No Outlier", "High-High Outlier", 
                 "Low-Low Outlier", "High-Low Outlier", 
                 "Low-High Outlier"), each=n+1),
  x = c(x_base, NA, x_outlier1, x_outlier2, x_outlier3, x_outlier4),
  y = c(y_base, NA, y_outlier1, y_outlier2, y_outlier3, y_outlier4),
  outlier = c(rep(FALSE, n), TRUE, rep(c(rep(FALSE, n), TRUE), 4))
)

# Remove the NA row
outlier_data <- outlier_data[!is.na(outlier_data$x),]

# Add correlation information
dataset_labels <- c(
  "No Outlier" = paste0("No Outlier (r = ", cor_base, ")"),
  "High-High Outlier" = paste0("High-High Outlier (r = ", cor1, ")"),
  "Low-Low Outlier" = paste0("Low-Low Outlier (r = ", cor2, ")"),
  "High-Low Outlier" = paste0("High-Low Outlier (r = ", cor3, ")"),
  "Low-High Outlier" = paste0("Low-High Outlier (r = ", cor4, ")")
)

# Create the visualization
ggplot(outlier_data, aes(x = x, y = y, color = outlier)) +
  geom_point(size = ifelse(outlier_data$outlier, 4, 2), alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "red", 
              aes(group = dataset), formula = y ~ x) +
  scale_color_manual(values = c("steelblue", "orangered")) +
  facet_wrap(~ dataset, ncol = 3, labeller = labeller(dataset = dataset_labels)) +
  labs(
    title = "Impact of Outlier Position on Correlation Coefficient",
    subtitle = "Outliers in opposite quadrants (lower right, upper left) most reduce correlation",
    caption = "Note how outliers in the same direction as the trend (upper-right, lower-left) strengthen r,\nwhile outliers against the trend direction (upper-left, lower-right) weaken r",
    x = "Variable X (e.g., Social Support Score)",
    y = "Variable Y (e.g., Well-being Index)"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 11),
    strip.text = element_text(size = 10, face = "bold")
  )
```

**Visual Analysis:** This plot demonstrates how outliers in different positions affect the correlation coefficient. Note how the high-low outlier (bottom right) dramatically reduces the correlation strength (from r=0.82 to r=0.28), as does the low-high outlier (top left, r=0.34). In contrast, high-high and low-low outliers actually strengthen the correlation (both to r=0.94). This visualization clearly shows that points contradicting the general positive trend have the greatest negative impact on correlation strength.

> **Hint:** Think of points that break the linear pattern.

1) Far bottom‚Äëleft (low X, very low Y)  

"1" = "‚ùå Incorrect. Points in the bottom-left (low X, low Y) actually strengthen positive correlations. The formula for Pearson's r involves multiplying deviations from the mean for both X and Y. When X is below its mean AND Y is below its mean, the product is positive, contributing positively to the correlation. The visualization shows this clearly‚Äîadding a low-low outlier increases r from 0.82 to 0.94. In research across disciplines, this position would represent cases that follow the general pattern but are at the extreme low end of both variables (e.g., in health research: low stress levels associated with low blood pressure). These cases reinforce rather than weaken the correlation pattern, making it inappropriate as an answer to what most reduces r."
2) Far bottom‚Äëright (high X, low Y)  

"2" = "‚úÖ Correct! A point in the bottom-right quadrant (high X, low Y) maximally reduces a positive correlation. This position represents a severe violation of the positive trend pattern, where X is far above its mean while Y is far below its mean. In the correlation formula, this creates a large negative product term that counteracts the positive relationship seen in the rest of the data. As shown in the visualization, this position dramatically reduces the correlation coefficient from r = 0.82 to r = 0.28. In any research field, being alert to such outliers is crucial‚Äîthey might represent special cases requiring explanation or potential data errors. The plot clearly demonstrates this effect, showing how a single contradictory point in this position substantially weakens the overall relationship."
3) Far top‚Äëright (high X, high Y)  

"3" = "‚ùå Incorrect. A point in the top-right (high X, high Y) actually strengthens a positive correlation rather than reducing it. When X and Y are both above their respective means, their deviations multiply to produce a positive contribution to the correlation coefficient. The visualization confirms this‚Äîadding a high-high outlier increases r from 0.82 to 0.94. Such points often represent cases that follow the general pattern but at extreme values (e.g., in educational research: high study time associated with high test scores). While these points may be influential and worth examining, they reinforce rather than diminish the correlation pattern. The visualization clearly shows how points following the trend direction‚Äîeven at extreme values‚Äîstrengthen rather than weaken the correlation."
4) A point exactly on the line  

"4" = "‚ùå Incorrect. A point that falls exactly on the regression line has almost no impact on the correlation coefficient, as it perfectly conforms to the linear relationship already established by the data. Such points are neither outliers nor influential observations‚Äîthey represent cases that follow the exact pattern predicted by the model. The correlation formula involves deviations from predicted values, and points on the line have zero or minimal deviation. In statistical analysis across all fields, observations that perfectly fit expected patterns are typically not concerning for data quality or influential case analysis. When evaluating outliers' impact on correlation, we should focus on points that deviate substantially from the general pattern, particularly those in quadrants that contradict the direction of association."

---

### Vraag Q20 (Analyze)
**Subgroup colors on the plot: what's a risk of looking only at overall r?**

```{r simpsons-paradox, echo=FALSE, fig.width=10, fig.height=5}
# Create simulated data to demonstrate Simpson's Paradox
set.seed(123)

# Group 1: Low poverty neighborhoods
n1 <- 30
x1 <- runif(n1, 1, 5)  # Police presence (1-5 units)
y1 <- 10 - 0.8*x1 + rnorm(n1, 0, 0.5)  # Crime rate (negative relationship)
group1 <- data.frame(
  Police_Presence = x1,
  Crime_Rate = y1,
  Group = "Low Poverty Areas",
  Poverty_Level = "Low"
)

# Group 2: High poverty neighborhoods
n2 <- 30
x2 <- runif(n2, 3, 8)  # Police presence (3-8 units)
y2 <- 15 - 0.8*x2 + rnorm(n2, 0, 0.5)  # Crime rate (negative relationship, but higher baseline)
group2 <- data.frame(
  Police_Presence = x2, 
  Crime_Rate = y2,
  Group = "High Poverty Areas",
  Poverty_Level = "High"
)

# Combine datasets
combined_data <- rbind(group1, group2)

# Calculate correlations
cor_group1 <- round(cor(x1, y1), 2)
cor_group2 <- round(cor(x2, y2), 2)
cor_combined <- round(cor(combined_data$Police_Presence, combined_data$Crime_Rate), 2)

# Create plot
ggplot(combined_data, aes(x = Police_Presence, y = Crime_Rate, color = Group)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1.2) +
  geom_smooth(data = combined_data, aes(x = Police_Presence, y = Crime_Rate), 
              inherit.aes = FALSE, method = "lm", color = "black", 
              linetype = "dashed", linewidth = 1.2, se = FALSE) +
  annotate("text", x = 2, y = 12, 
           label = paste0("Within Low Poverty Areas: r = ", cor_group1, " (negative correlation)"),
           hjust = 0, color = "darkgreen", fontface = "bold", size = 4) +
  annotate("text", x = 2, y = 11, 
           label = paste0("Within High Poverty Areas: r = ", cor_group2, " (negative correlation)"),
           hjust = 0, color = "darkred", fontface = "bold", size = 4) +
  annotate("text", x = 2, y = 10, 
           label = paste0("Overall Combined: r = ", cor_combined, " (POSITIVE correlation)"),
           hjust = 0, color = "black", fontface = "bold", size = 4) +
  labs(
    title = "Simpson's Paradox in Correlation Analysis",
    subtitle = "When subgroups show opposite pattern from the overall data",
    x = "Police Presence",
    y = "Crime Rate",
    color = "Neighborhood Type"
  ) +
  theme_minimal() +
  theme(
    legend.position = "top",
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12)
  ) +
  scale_color_manual(values = c("darkred", "darkgreen"))
```

**Simpson's Paradox Explained**: In this example, police presence is negatively correlated with crime within both low-poverty and high-poverty neighborhoods (more police ‚Üí less crime). However, when the groups are combined, the overall correlation becomes positive, falsely suggesting more police ‚Üí more crime. This occurs because high-poverty areas both have more police presence AND higher crime rates. Without accounting for neighborhood poverty levels, we would reach an incorrect conclusion about the relationship between policing and crime.

> **Hint:** Think Simpson's paradox and heterogeneity across groups.

1) Subgroup patterns can mask or reverse the overall pattern  
"1" = "‚úÖ Correct! This illustrates Simpson's paradox, where the direction of association within subgroups differs from or even reverses the overall trend. In this example, police presence is negatively correlated with crime rates within both high and low poverty neighborhoods (showing that more police presence is associated with lower crime), but the overall correlation becomes positive when data are combined. This happens because high-poverty neighborhoods have both higher crime rates AND more police presence compared to low-poverty areas. In criminology, ignoring important group distinctions like neighborhood characteristics, demographic factors, or jurisdictional differences can lead to fundamentally wrong conclusions about policy effectiveness. Always stratify analysis by relevant groups when heterogeneity is suspected, and use visualization to identify potential Simpson's paradox situations."

2) r always changes when you add colors  
"2" = "‚ùå Incorrect. The correlation coefficient (r) is calculated mathematically from the data values, regardless of how points are colored or visualized in a plot. Colors in a scatterplot are visual aids that help identify patterns or group membership, but they don't directly affect the numerical calculation of correlation. The issue demonstrated in this plot is not about color affecting r, but about how aggregating heterogeneous subgroups (which happen to be shown in different colors) can produce misleading overall correlations. The fundamental problem is that important group distinctions are being ignored in the overall analysis, not that the visualization includes colors. Proper statistical analysis involves identifying relevant subgroups and analyzing relationships within them, rather than relying solely on aggregate measures."

3) r can never be misleading  
"3" = "‚ùå Incorrect. The correlation coefficient r can be highly misleading when important contextual factors or group differences are ignored, as demonstrated by Simpson's paradox in this example. Other ways r can mislead include: (1) When nonlinear relationships exist but r only measures linear association; (2) When outliers distort the overall relationship; (3) When the relationship is driven by a few influential points; (4) When r is statistically significant but practically meaningless due to very large sample sizes; or (5) When temporal, spatial, or hierarchical dependencies exist in the data. In criminology, correlation coefficients should always be accompanied by visual inspection of the data, examination of subgroups, and consideration of potential confounding variables to avoid misleading conclusions that could impact policy decisions."

4) Subgroups automatically increase r  
"4" = "‚ùå Incorrect. There is no general rule that subgroups increase correlation. In fact, as shown in this example, the individual subgroups (r = -0.76 and r = -0.75) both have stronger correlations (albeit negative) than the combined data (r = 0.61). The relationship between subgroup correlations and the overall correlation depends on many factors: the positions of the subgroups relative to each other, the variance within each subgroup, the sample sizes of the subgroups, and whether the relationships within subgroups are in the same or opposite directions as the overall relationship. In criminological research, properly accounting for relevant subgroups often provides more accurate and nuanced understanding of relationships compared to aggregate analysis, but this can either strengthen or weaken correlation depending on the specific data structure."

---

## EVALUATE LEVEL (4 vragen)

### Vraag Q21 (Evaluate)
**Two criminology studies show conflicting results about the relationship between poverty and crime rates. Study A (N=50) reports r = 0.72, p < 0.001. Study B (N=5000) reports r = 0.18, p < 0.001. Which study provides more trustworthy evidence?**

```{r conflicting-studies, echo=FALSE, fig.width=10, fig.height=5}
# Create comparative data for visualization
study_comparison <- data.frame(
  Study = c("Study A", "Study B"),
  Sample_Size = c(50, 5000),
  Correlation = c(0.72, 0.18),
  p_value = c("<0.001", "<0.001"),
  Variance_Explained = c(0.72^2, 0.18^2),
  CI_Lower = c(0.55, 0.15),  # Approximate confidence intervals
  CI_Upper = c(0.83, 0.21)
)

# Calculate values for visualization
study_comparison$Variance_Explained <- round(study_comparison$Variance_Explained * 100, 1)

# Create a comparison bar chart
p1 <- ggplot(study_comparison, aes(x = Study, y = Correlation, 
                              fill = Study)) +
  geom_bar(stat = "identity", width = 0.6) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), 
                width = 0.2, linewidth = 1) +
  scale_fill_manual(values = c("darkblue", "steelblue")) +
  labs(
    title = "Correlation Strength Comparison",
    subtitle = "With 95% confidence intervals",
    y = "Correlation Coefficient (r)",
    x = NULL
  ) +
  ylim(0, 1) +
  geom_text(aes(label = paste0("r = ", Correlation)), 
            position = position_dodge(width = 0.9), 
            vjust = -0.5, color = "black", size = 4) +
  theme_minimal()

# Create a variance explained chart
p2 <- ggplot(study_comparison, aes(x = Study, y = Variance_Explained, 
                              fill = Study)) +
  geom_bar(stat = "identity", width = 0.6) +
  scale_fill_manual(values = c("darkblue", "steelblue")) +
  labs(
    title = "Variance Explained (R¬≤)",
    subtitle = "Percentage of variance accounted for",
    y = "Variance Explained (%)",
    x = NULL
  ) +
  geom_text(aes(label = paste0(Variance_Explained, "%")), 
            position = position_dodge(width = 0.9), 
            vjust = -0.5, color = "black", size = 4) +
  theme_minimal()

# Create a sample size comparison (log scale)
p3 <- ggplot(study_comparison, aes(x = Study, y = Sample_Size, 
                              fill = Study)) +
  geom_bar(stat = "identity", width = 0.6) +
  scale_fill_manual(values = c("darkblue", "steelblue")) +
  scale_y_log10() +
  labs(
    title = "Sample Size Comparison",
    subtitle = "Log scale",
    y = "Sample Size (N)",
    x = NULL
  ) +
  geom_text(aes(label = paste0("N = ", Sample_Size)), 
            position = position_dodge(width = 0.9), 
            vjust = -0.5, color = "black", size = 4) +
  theme_minimal()

# Combine the plots using cowplot or patchwork
library(gridExtra)
grid.arrange(p1, p2, ncol = 2)
```

**Critical Evaluation Factors:**
1. Study B has a much larger sample size (5000 vs 50)
2. Study A shows a much stronger effect (r = 0.72 vs r = 0.18)
3. Both are statistically significant (p < 0.001)
4. Study A explains 51.8% of variance vs. only 3.2% in Study B
5. Study B likely offers better external validity due to larger, more representative sample

> **Hint:** Consider sample size, effect size, statistical significance, and practical significance.

1) Study A because it has a stronger correlation  
"1" = "‚ùå Incorrect. While Study A shows a stronger correlation (r = 0.72 vs r = 0.18), correlation strength alone doesn't determine trustworthiness. The small sample size (N=50) in Study A is problematic because small samples are more vulnerable to sampling error and can produce unstable, unreliable estimates. In criminological research, strong correlations from small samples often fail to replicate in larger, more representative samples. This pattern is called the 'decline effect' ‚Äî impressive initial findings that diminish when studied more rigorously. Additionally, small samples can be unduly influenced by a few extreme cases or selection bias. The seemingly impressive correlation in Study A should be viewed with appropriate skepticism."

2) Study B because of the much larger sample size providing more reliable estimates  
"2" = "‚úÖ Correct! Large N gives more stable, generalizable estimates despite smaller effect. Study B's sample size (N=5000) provides much greater statistical power and precision. With such a large sample, even small correlations like r = 0.18 can be reliable and meaningful in criminological contexts. The 95% confidence interval around this estimate will be much narrower than for Study A, indicating greater precision. Furthermore, large samples are more likely to represent the true population diversity, reducing selection bias. In criminology, where policy decisions affect many lives, we should prioritize large, representative samples over impressive-looking correlations from small studies. Study B likely captures the true relationship between poverty and crime more accurately, even though the effect appears smaller."

3) Both are equally valid since both are statistically significant  
"3" = "‚ùå Incorrect. Statistical significance doesn't guarantee practical importance or reliability. While both studies show p < 0.001, this only tells us the results are unlikely due to random chance ‚Äî it says nothing about the quality of sampling, measurement, or generalizability. In criminological research, it's relatively easy to achieve statistical significance with poor methodology, especially in small samples with large effects (Study A) or large samples with small effects (Study B). The fact that both are 'significant' tells us little about their relative trustworthiness. Statistical significance should never be the sole criterion for evaluating evidence quality. Instead, methodological rigor, sample representativeness, and replicability are far more important considerations."
4) Neither because correlational studies can't establish causation  

"4" = "‚ùå Incorrect. While true about causation, the question asks about trustworthy evidence for the relationship strength. Both studies provide evidence about the association between poverty and crime, even if they can't definitively establish causation. In criminology, correlational evidence is still valuable for understanding patterns and generating hypotheses, particularly when experimental designs are impractical or unethical (we can't randomly assign people to poverty conditions). The limitation regarding causation applies equally to both studies and doesn't help us evaluate which provides more trustworthy evidence about the strength of the relationship. Study B's large sample still makes it more trustworthy for estimating the correlation magnitude, regardless of causal implications."

---

---

### Vraag Q22 (Evaluate)
**A news article claims: "Strong correlation (r = 0.65) between neighborhood watch programs and reduced burglary proves these programs are highly effective crime prevention tools." Evaluate this statistical claim.**

> **Hint:** Consider correlation vs. causation, confounding variables, and research design limitations.

1) Valid conclusion - strong correlation provides sufficient evidence of effectiveness  
"1" = "‚ùå Incorrect. Correlation alone cannot establish causal effectiveness. Even a strong correlation coefficient (r = 0.65) only tells us the variables are associated, not that one causes the other. For instance, neighborhoods with stronger community cohesion might independently have both more neighborhood watch participation AND lower crime rates due to informal social control. In criminology, policy recommendations require stronger causal evidence than mere correlation, typically from experimental or quasi-experimental designs (like randomized controlled trials or regression discontinuity). The news article makes a fundamental error in statistical interpretation that could lead to misallocated crime prevention resources."

2) Invalid - correlation doesn't prove causation; neighborhood characteristics might explain both variables  
"2" = "‚úÖ Correct! Confounders like socioeconomic status could influence both program presence and crime rates. This is a classic example of potential confounding in criminological research. Wealthier neighborhoods often have both more resources to establish watch programs AND lower baseline crime rates. Other possible confounders include: population density, residential stability, police presence, or architectural features. Without controlling for these variables using methods like multiple regression or propensity score matching, we cannot attribute crime reduction to the watch programs. Additionally, the direction of causality could be reversed‚Äîneighborhoods with lower crime rates might have more residents willing to participate in watch programs, rather than the programs causing the lower crime."

3) Invalid - r = 0.65 is too weak to support any conclusions  
"3" = "‚ùå Incorrect. The correlation strength isn't the main issue; causation vs. correlation is. In fact, r = 0.65 would generally be considered a strong correlation in criminological research. The fundamental problem with the news article's claim isn't that the correlation is too weak, but that correlation‚Äîregardless of strength‚Äîcannot establish causation. Even a perfect correlation (r = 1.00) wouldn't prove that neighborhood watch programs cause crime reduction. Evidence-based criminology requires distinguishing between correlation and causation through appropriate research methodology, including randomized trials, natural experiments, instrumental variables, or statistical controls for potential confounders."

4) Valid if the sample size was large enough  
"4" = "‚ùå Incorrect. Sample size doesn't resolve the causation problem. A larger sample would increase statistical power and provide more precise correlation estimates, but it cannot transform a correlational finding into a causal one. The logical fallacy in the news article's claim‚Äîthat correlation implies causation‚Äîpersists regardless of sample size. In criminological research, sample size matters for reliability and generalizability, but the research design determines whether causal claims are warranted. Without appropriate design features like randomization, time-order establishment, or control for confounding variables, even studies with enormous samples cannot support causal conclusions about program effectiveness."

---

### Vraag Q23 (Evaluate)
**You're reviewing a study that found r = -0.43 between education level and arrest rates. The scatterplot shows clear clustering of points with some extreme outliers. How should you evaluate the reported correlation?**

> **Hint:** Consider robustness of correlation to outliers and alternative measures.

```{r robust-correlation, echo=FALSE, fig.width=10, fig.height=5}
# Generate data with outliers
set.seed(42)
n <- 40
education <- round(runif(n, 8, 18), 1)  # Education years (8-18)
arrests_base <- 10 - 0.5*education + rnorm(n, 0, 1)  # Negative relationship
arrests <- pmax(0, arrests_base)  # Ensure no negative arrests

# Add outliers
education_with_outliers <- c(education, 6, 7, 19)
arrests_with_outliers <- c(arrests, 12, 10, 7)  # Outliers that affect correlation

# Calculate correlations
pearson_r <- round(cor(education_with_outliers, arrests_with_outliers, method = "pearson"), 2)
spearman_r <- round(cor(education_with_outliers, arrests_with_outliers, method = "spearman"), 2)
kendall_r <- round(cor(education_with_outliers, arrests_with_outliers, method = "kendall"), 2)

# Create data frame for plotting
plot_data <- data.frame(
  Education = education_with_outliers,
  Arrests = arrests_with_outliers,
  Point_Type = c(rep("Regular Data Point", n), rep("Outlier", 3))
)

# Create scatter plot with regression line
p1 <- ggplot(plot_data, aes(x = Education, y = Arrests, color = Point_Type, shape = Point_Type)) +
  geom_point(size = 3, alpha = 0.8) +
  geom_smooth(method = "lm", se = TRUE, color = "red", formula = y ~ x) +
  scale_color_manual(values = c("orangered", "steelblue")) +
  scale_shape_manual(values = c(16, 17)) +
  labs(
    title = "Education Level vs. Arrest Rate with Outliers",
    subtitle = paste0("Pearson r = ", pearson_r, " (affected by outliers)"),
    x = "Education Level (years)",
    y = "Arrest Rate (per 100 individuals)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank())

# Create a comparison table
comparison_data <- data.frame(
  Correlation_Type = c("Pearson (parametric)", "Spearman (rank-based)", "Kendall's Tau (rank-based)"),
  Coefficient = c(pearson_r, spearman_r, kendall_r),
  Robustness = c("Low - Sensitive to outliers", "High - Based on ranks", "High - Based on concordant pairs"),
  When_to_Use = c("Linear relationships, normal data", "Monotonic relationships, ordinal data", "Small samples, tied ranks")
)

# Create table
library(kableExtra)
table_plot <- kable(comparison_data, caption = "Comparison of Correlation Methods", booktabs = TRUE)

# Display plot
p1
```

**Analysis:** The plot shows how outliers (orange triangles) affect the Pearson correlation coefficient. While Pearson's r = -0.43, Spearman's rank correlation is -0.67, suggesting a stronger relationship when outlier influence is reduced. When evaluating correlational studies, always examine scatterplots for outliers and consider robust correlation metrics.

1) Accept r = -0.43 as accurate since it's statistically significant  
"1" = "‚ùå Incorrect. Outliers can dramatically affect Pearson correlation. Pearson's correlation coefficient (r) assumes a linear relationship and normal distribution of variables, making it highly sensitive to extreme values. In this case, the three outliers visible in the scatterplot (the orange triangles) are likely pulling the correlation coefficient toward zero from what would be a stronger negative relationship. Statistical significance only tells us the relationship is unlikely to be due to chance; it doesn't validate the choice of correlation method. In criminological research, outliers might represent important special cases (e.g., extremely high-education individuals with criminal records), but their disproportionate mathematical impact on Pearson's r can obscure the general pattern applicable to the majority of cases."

2) Question the correlation's robustness; recommend reporting both Pearson and Spearman correlations  
"2" = "‚úÖ Correct! Outliers can distort Pearson r; Spearman correlation would be more robust to extreme values. By converting data to ranks, Spearman's correlation (œÅ or rho) reduces the influence of outliers and provides a more accurate measure of the monotonic relationship between education and arrest rates. In this case, Spearman's correlation (-0.67) reveals a stronger negative relationship than Pearson's (-0.43), suggesting that education's protective effect against arrests may be understated by the Pearson coefficient. Reporting both metrics is considered best practice in criminological research when data contain outliers or show non-linear relationships. This approach provides transparency and allows readers to understand how sensitive the findings are to analytical choices. Additionally, visualizing the data with scatterplots (as shown) helps others evaluate the distribution pattern and appropriateness of different correlation methods."

3) Reject the entire analysis because outliers invalidate all correlational studies  
"3" = "‚ùå Incorrect. Outliers don't invalidate analysis but require appropriate handling. While outliers affect correlation coefficients, they don't render the entire analysis worthless. Instead, they signal the need for robust statistical approaches. In criminology, outliers often represent meaningful cases (e.g., individuals with unique characteristics or circumstances) rather than measurement errors. Proper approaches include: (1) using robust correlation methods like Spearman's or Kendall's tau; (2) reporting results both with and without outliers; (3) transforming variables to reduce skew; or (4) using regression techniques with robust standard errors. Complete rejection of the analysis would waste valuable data and insights. Instead, criminologists should acknowledge outliers' presence, explore their influence, and select appropriate analytical strategies that account for their effect."

4) The correlation is definitely too weak to be meaningful regardless of outliers  
"4" = "‚ùå Incorrect. r = -0.43 represents a moderate relationship that could be practically important. In criminological research, even moderate correlations can have substantial real-world implications, especially for policy decisions. A correlation of -0.43 indicates that approximately 18% of the variance in arrest rates is explained by education level‚Äîa notable finding suggesting that educational interventions might help reduce criminal behavior. Furthermore, the strength of correlations must be interpreted within their disciplinary context. In criminology, where many factors influence complex social behaviors, correlations above 0.3 are often considered meaningful. When properly analyzed with robust methods that account for outliers (like the Spearman correlation of -0.67 shown in the analysis), the relationship appears even stronger. Dismissing moderate correlations as 'too weak' risks overlooking important criminological relationships."

---

## CREATE LEVEL (3 vragen)

### Vraag Q24 (Create)
**Design a correlational study to investigate the relationship between social media usage and antisocial behavior in adolescents. What key design elements would you include?**

> **Hint:** Consider variables, measurement, sampling, controls, and ethical considerations.

1) Survey 100 teenagers about hours of social media use and self-reported antisocial acts  
"1" = "‚ùå Incomplete. This design has several methodological weaknesses: (1) The sample size (N=100) is relatively small for detecting potentially subtle effects in behavioral research; (2) Simple self-reported hours fails to capture the complex, multidimensional nature of social media use (different platforms, active vs. passive use, content types, etc.); (3) No control variables means any observed correlation could be due to numerous confounding factors (socioeconomic status, parental supervision, personality traits, etc.); (4) Cross-sectional design prevents establishing temporal precedence, which is necessary for causal inference; (5) No validation measures for self-reported antisocial behavior increases measurement error. A robust correlational study requires addressing these methodological challenges."

2) Longitudinal design measuring social media use (multiple platforms, time), antisocial behavior (multiple measures), controlling for age, SES, and family factors, with proper ethical oversight  
"2" = "‚úÖ Correct! This comprehensive design incorporates key methodological strengths: (1) Longitudinal measurement allows tracking changes over time and establishing temporal precedence (did social media use precede behavioral changes?); (2) Measuring multiple aspects of social media use (time spent, platform types, content engagement) captures the complexity of digital behavior; (3) Using multiple behavioral measures strengthens construct validity and reduces measurement error; (4) Controlling for important covariates like age, socioeconomic status, and family factors helps rule out alternative explanations; (5) Ethical oversight ensures participant protection, informed consent, data privacy, and appropriate risk management. This design maximizes the validity of potential correlational findings while minimizing methodological weaknesses."

3) Experimental manipulation where some teens are assigned high social media use  
"3" = "‚ùå Inappropriate. While experimental designs can establish causality more definitively than correlational designs, this approach has serious flaws: (1) Ethical concerns with potentially harmful manipulation‚Äîresearch ethics principles prohibit exposing participants, especially minors, to interventions that might increase risk for negative outcomes; (2) External validity problems, as artificially assigned social media use differs from natural, self-selected use patterns; (3) Ecological validity issues, since controlled experimental conditions poorly represent real-world social media engagement; (4) Retention challenges in maintaining compliance with assigned conditions over meaningful timeframes; (5) The question specifically asked for a correlational design, not an experimental one. Experimental designs have their place, but must be implemented ethically and appropriately."

4) Simple correlation between total screen time and police contacts  
"4" = "‚ùå Too simplistic. This approach has multiple conceptual and methodological weaknesses: (1) Total screen time is too general a measure, encompassing everything from homework to video games, not specifically social media use; (2) Police contacts represent only the most extreme and detected antisocial behaviors, missing the vast majority of minor and undetected behaviors; (3) Both measures are highly susceptible to selection biases, measurement error, and third-variable problems; (4) No consideration of important moderating factors (age, gender, socioeconomic context) that affect both variables; (5) Lacks measurement validity for both the independent and dependent variables. The resulting correlation would be difficult to interpret meaningfully due to these fundamental limitations."

---

### Vraag Q25 (Create)
**Formulate a testable research hypothesis about the relationship between economic inequality and violent crime rates at the city level, including predicted direction and strength.**

> **Hint:** Create a specific, directional hypothesis that can be tested with correlation/regression.

```{r hypothesis-visualization, echo=FALSE, fig.width=10, fig.height=6}
# Create simulated data for visualization
set.seed(123)
n_cities <- 50

# Generate income inequality index (Gini coefficient range: 0.3-0.6)
gini <- runif(n_cities, 0.3, 0.6)

# Generate violent crime rates with moderate-to-strong correlation with Gini
violent_crime <- 2 + 25*gini + rnorm(n_cities, 0, 3)

# Create city labels
city_names <- paste("City", 1:n_cities)

# Create data frame
city_data <- data.frame(
  City = city_names,
  Income_Inequality = gini,
  Violent_Crime_Rate = violent_crime
)

# Calculate correlation
correlation <- round(cor(gini, violent_crime), 2)
r_squared <- round(correlation^2 * 100, 1)

# Create plot
ggplot(city_data, aes(x = Income_Inequality, y = Violent_Crime_Rate)) +
  geom_point(color = "steelblue", size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", formula = y ~ x, se = TRUE, color = "darkred") +
  labs(
    title = "Hypothesized Relationship: Economic Inequality and Violent Crime",
    subtitle = paste0("Example of moderate-strong positive correlation (r = ", 
                     correlation, ", R¬≤ = ", r_squared, "%)"),
    x = "Economic Inequality (Gini Coefficient)",
    y = "Violent Crime Rate (per 100,000 population)",
    caption = "Visualization of a testable research hypothesis showing predicted direction and strength"
  ) +
  annotate("text", x = 0.32, y = max(violent_crime) - 1, 
           label = paste0("Research Hypothesis Example:\n",
                          "Cities with higher income inequality (Gini coefficient)\n", 
                          "will show moderately strong positive correlations\n",
                          "(r = 0.40-0.60) with violent crime rates per capita"),
           hjust = 0, size = 4, fontface = "bold") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(size = 11)
  )
```

**Hypothesis Formulation Elements:**
1. **Variables:** Specifically defined and measurable (Gini coefficient, violent crime rate per 100,000)
2. **Direction:** Clear prediction (positive correlation)
3. **Magnitude:** Expected effect size range (r = 0.40-0.60)
4. **Unit of analysis:** Specified level (city-level)
5. **Testability:** Can be confirmed/rejected with statistical analysis

1) "Economic inequality affects crime somehow"  
"1" = "‚ùå Too vague. This statement fails as a testable hypothesis for several reasons: (1) It lacks directionality‚Äîdoesn't specify whether inequality increases or decreases crime; (2) No operationalization of key variables‚Äîhow is economic inequality measured? What type of crime?; (3) No unit of analysis‚Äîat what level (individual, neighborhood, city, country) is this relationship being examined?; (4) No predicted effect size or relationship strength; (5) No temporal scope is specified. A good research hypothesis needs precision in all these dimensions. Without such specificity, the claim remains a general statement that cannot be meaningfully tested with correlation or regression analysis, as it's impossible to determine what would constitute evidence for or against it."

2) "Cities with higher income inequality (Gini coefficient) will show moderately strong positive correlations (r = 0.40-0.60) with violent crime rates per capita"  
"2" = "‚úÖ Correct! This hypothesis exemplifies all key elements of a well-formulated, testable research prediction: (1) Clear specification of variables‚Äîincome inequality measured by the Gini coefficient and violent crime rates per capita; (2) Explicit directionality‚Äîa positive correlation is predicted; (3) Defined magnitude‚Äîmoderately strong correlation in the range of r = 0.40-0.60; (4) Appropriate unit of analysis‚Äîcity-level comparison; (5) Implied research design‚Äîcorrelational analysis across multiple cities. This precise formulation allows for clear testing using statistical methods, with specific criteria for confirmation or rejection. The inclusion of an expected effect size range demonstrates sophisticated understanding of both the statistical methods and the substantive research area, allowing for meaningful interpretation of results regardless of the field of study."

3) "Poor people commit more violent crimes"  
"3" = "‚ùå Individual-level claim, inappropriate for several reasons: (1) This statement makes a causal claim about individual behavior rather than a correlation at the city level as requested; (2) It conflates economic status with inequality‚Äîpoverty and inequality are related but distinct concepts; (3) It implies a direct causal relationship rather than a correlation; (4) The unit of analysis (individuals) doesn't match what was requested (city-level); (5) No measurement operationalization is provided‚Äîhow is poverty defined? How is crime measured? The fundamental mismatch between this individual-level causal claim and the requested city-level correlation hypothesis makes this answer incorrect, regardless of whether the underlying claim might be supported by other research approaches."

4) "Economic inequality is the sole cause of all violent crime"  
"4" = "‚ùå Too extreme and untestable. This statement fails as a hypothesis because: (1) It makes an absolute causal claim‚Äîidentifying inequality as the 'sole cause'‚Äîrather than proposing a correlation; (2) It uses universal language ('all violent crime') that would be invalidated by a single counterexample; (3) It lacks any specification of measurement for either variable; (4) It proposes a perfect relationship that contradicts the complexity of human behavior; (5) It cannot be meaningfully tested with correlation or regression analysis, as correlation coefficients, by nature, cannot establish sole causation. In any scientific field, hypotheses making absolute claims about sole causation are generally considered too extreme and fail to acknowledge the multivariate nature of complex phenomena."

---
